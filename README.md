# Exercice_1_ML

In this exercice, we did a split of the Iris dataset into a training set and a test set.
We observed the behavior of the two Optimizers and we can tell that loss is way more smaller with Adam than with SGD.
We note that the batch size and learning rate really affect the performance of the Optimizer and we can also note that Adam is faster and gives a smaller loss than SGD.
